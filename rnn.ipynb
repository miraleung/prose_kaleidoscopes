{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d20a8cb8",
   "metadata": {},
   "source": [
    "# Prose Kaleidoscopes - RNN Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d71a496-2531-46f5-8e55-d45aa7755617",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove/glove.6B.50d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-267e4d042964>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mGLOVE_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"glove/glove.6B.50d.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mGLOVE_LOOKUP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcs224u_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglove2dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGLOVE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/scu/xcs224u/project/prose_kaleidoscopes/cs224u_utils.py\u001b[0m in \u001b[0;36mglove2dict\u001b[0;34m(src_filename)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mline_parser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove/glove.6B.50d.txt'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch_rnn_classifier import TorchRNNClassifier\n",
    "from torch_tree_nn import TorchTreeNN\n",
    "import sst\n",
    "import cs224u_utils\n",
    "\n",
    "# If Jupyter complains that torch is an unknown module, try:\n",
    "# conda install pytorch torchvision -c pytorch\n",
    "import torch\n",
    "\n",
    "# Transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "from transformers import BertModel\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Own files\n",
    "import dataset_io\n",
    "\n",
    "\n",
    "# These params should never change\n",
    "NUM_EPOCHS = 4 \n",
    "SEED = 42\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 4e-5\n",
    "NUM_WARMUP_STEPS = 100\n",
    "MAX_SEQ_LEN = 512\n",
    "\n",
    "# Constants\n",
    "DATASET_TYPE_IMDB = \"imdb\"\n",
    "DATASET_TYPE_AMAZON = \"amazon\"\n",
    "DATASET_TYPE_SST2 = \"sst2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5591b417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "# Retrieve GloVe pretrained vectors.\n",
    "\n",
    "GLOVE_DIR = \"glove\"\n",
    "GLOVE_ZIP_FILE = \"{0}/glove.6B.zip\".format(GLOVE_DIR)\n",
    "GLOVE_PATH = \"{0}/glove.6B.50d.txt\".format(GLOVE_DIR)\n",
    "GLOVE_FILE_EXISTS = os.path.exists(GLOVE_PATH)\n",
    "\n",
    "if not os.path.exists(GLOVE_DIR):\n",
    "    GLOVE_FILE_EXISTS = False\n",
    "    os.mkdir(GLOVE_DIR)\n",
    "\n",
    "if not GLOVE_FILE_EXISTS:\n",
    "    # Works on both Mac and Linux. If you are on Windows,\n",
    "    # please download the zip file manually.\n",
    "    if not os.path.exists(GLOVE_ZIP_FILE):\n",
    "        !cd {GLOVE_DIR}; curl -LO http://nlp.stanford.edu/data/glove.6B.zip\n",
    "    !cd {GLOVE_DIR}; unzip glove.6B.zip\n",
    "\n",
    "\n",
    "GLOVE_LOOKUP = cs224u_utils.glove2dict(GLOVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d70663e-fe48-46b2-a4ee-e0c450051b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_list_labels(lst, dataset_type):\n",
    "    if dataset_type == DATASET_TYPE_IMDB:\n",
    "        # Labels are either \"positive\" or \"negative\"\n",
    "        return [0 if v == \"negative\" else 1 for v in lst]\n",
    "    if dataset_type == DATASET_TYPE_AMAZON:\n",
    "        # Labels are in the range [1, 5]\n",
    "        return [(v - 1) * (1.0 / 5) for v in lst]\n",
    "    # SST-2 are 0 (negative) or 1 (positive).\n",
    "    return lst\n",
    "\n",
    "def normalize_labels(df, label_field):\n",
    "    labels = getattr(df, label_field).to_numpy()\n",
    "    # Fix IMDB labels.\n",
    "    is_imdb = len(set(labels)) == 2 and not str(labels[0]).isdigit() and \"positive\" in labels and \"negative\" in labels\n",
    "    # Fix Amazon labels.\n",
    "    is_amazon = len(set(labels)) == 5 and min(set(labels)) == 1 and max(set(labels)) == 5\n",
    "    if is_imdb:\n",
    "        return pd.get_dummies(labels)[\"positive\"]  # 0 is neg, 1 is pos\n",
    "    if is_amazon:\n",
    "        return labels - 1\n",
    "    return labels\n",
    "\n",
    "def vsm_phi(text, lookup, np_func=np.mean):\n",
    "    allvecs = np.array([lookup[w] for w in text.split() if w in lookup])\n",
    "    if len(allvecs) == 0:\n",
    "        dim = len(next(iter(lookup.values())))\n",
    "        feats = np.zeros(dim)\n",
    "    else:\n",
    "        feats = np_func(allvecs, axis=0)\n",
    "    return feats\n",
    "\n",
    "def glove_phi(text, np_func=np.mean):\n",
    "    return vsm_phi(text, glove_lookup, np_func=np_func)\n",
    "\n",
    "def simple_leaves_phi(text):\n",
    "    return text.split()\n",
    "\n",
    "#  {'embed_dim': 50, 'eta': 0.005, 'hidden_dim': 100}\n",
    "def fit_rnn_classifier(X, y):\n",
    "    #print(y)\n",
    "    basemod = TorchRNNClassifier(\n",
    "        GLOVE_VOCAB,\n",
    "        embedding=GLOVE_EMBEDDING,\n",
    "        batch_size=25,\n",
    "        embed_dim=50,\n",
    "        hidden_dim=100,\n",
    "        bidirectional=True,\n",
    "        early_stopping=True, \n",
    "        eta=0.005)\n",
    "    basemod.fit(X, y)\n",
    "    return basemod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5110593c-847c-49a5-8a14-52461d91667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "def get_roc_metrics(probs, y_true, num_classes, dataset_type):\n",
    "    preds = np.array(probs)\n",
    "    if dataset_type != DATASET_TYPE_AMAZON:\n",
    "        fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Get accuracy over the test set\n",
    "        y_pred = np.where(preds >= 1.0 / num_classes, 1, 0)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        return { \n",
    "            'fpr' : fpr, \n",
    "            'tpr' : tpr, \n",
    "            'roc_auc' : roc_auc, \n",
    "            'accuracy' : accuracy\n",
    "        }\n",
    "    \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    # Force any empty categories to be present\n",
    "    cat_preds = pd.DataFrame(preds)\n",
    "    cat_preds = cat_preds.astype(pd.CategoricalDtype(categories=list([round(i / num_classes, 1) for i in range(NUM_CLASSES)])))\n",
    "    y_true = pd.get_dummies(y_true).to_numpy()\n",
    "    preds = pd.get_dummies(cat_preds).to_numpy()\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], preds[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), preds.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(num_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= NUM_CLASSES\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "    y_pred = np.where(preds >= 1.0 / num_classes, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    metrics = {'accuracy' : accuracy}\n",
    "    for k in fpr.keys():\n",
    "        metrics[\"fpr_{0}\".format(k)] = fpr[k]\n",
    "        metrics[\"tpr_{0}\".format(k)] = tpr[k]\n",
    "        metrics[\"roc_auc_{0}\".format(k)] = roc_auc[k]\n",
    "    return metrics\n",
    "    \n",
    "def evaluate_roc_twoclass(probs, y_true, num_classes, dataset_type):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    fpr, tpr, roc_auc, accuracy = list(get_roc_metrics(probs, y_true, num_classes, dataset_type).values())\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "    print(f'Accuracy: {accuracy*100:.4f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    \n",
    "def evaluate_roc_multiclass(probs, y_true, num_classes, dataset_type):\n",
    "    metrics = get_roc_metrics(probs, y_true, num_classes, dataset_type)\n",
    "    ks = [\"macro\", \"micro\"] + [i for i in range(NUM_CLASSES)]\n",
    "    fpr = { k : metrics[\"fpr_{0}\".format(k)] for k in ks }\n",
    "    tpr = { k : metrics[\"tpr_{0}\".format(k)] for k in ks }\n",
    "    roc_auc = { k : metrics[\"roc_auc_{0}\".format(k)] for k in ks }\n",
    "    accuracy = metrics['accuracy']\n",
    "    \n",
    "    print(f'AUC: {0}', roc_auc)\n",
    "    print(f'Accuracy: {accuracy*100:.4f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"], 'b', label = 'Micro-avg AUC = %0.2f' % roc_auc[\"micro\"], color = 'navy')\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"], 'b', label = 'Macro-avg AUC = %0.2f' % roc_auc[\"macro\"], color = 'darkviolet')\n",
    "    colors = ['orange', 'forestgreen', 'cornflowerblue', 'darkgoldenrod', 'tomato', 'dodgerblue']\n",
    "    lw=2\n",
    "    for i, color in zip(range(NUM_CLASSES), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw, label='AUC of class {0} = {1:0.2f})'.format(i, roc_auc[i]))\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_roc(probs, y_true, num_classes, dataset_type):\n",
    "    if dataset_type == DATASET_TYPE_AMAZON:\n",
    "        evaluate_roc_multiclass(probs, y_true, num_classes, dataset_type)\n",
    "    else:\n",
    "        evaluate_roc_twoclass(probs, y_true, num_classes, dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c12b018-ea0e-4078-9562-bf639cc7e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution and Metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, cohen_kappa_score, roc_auc_score\n",
    "\n",
    "def get_pred(value, num_classes):\n",
    "    interval = 1.0 / num_classes\n",
    "    for i in range(0, num_classes):\n",
    "        if value < (i + 1) * interval:\n",
    "            return i\n",
    "    return num_classes - 1\n",
    "\n",
    "def run_and_eval(train, test, text_field, label_field, test_labels, num_classes, dataset_type):\n",
    "    # Evaluate on test set\n",
    "    rnn_experiment = sst.experiment(\n",
    "        train,\n",
    "        simple_leaves_phi, \n",
    "        fit_rnn_classifier,\n",
    "        assess_dataframes=[test], \n",
    "        vectorize=False,\n",
    "        text_field=text_field,\n",
    "        label_field=label_field)\n",
    "    \n",
    "    test_probs = rnn_experiment['predictions'][0]\n",
    "    test_probs_norm = normalize_list_labels(test_probs, dataset_type)\n",
    "\n",
    "    # Evaluate the classifier\n",
    "    print(\"Test Set\")\n",
    "    evaluate_roc(test_probs_norm, test_labels, num_classes, dataset_type)\n",
    "    \n",
    "    test_preds = np.array([get_pred(p, num_classes) for p in test_probs_norm])\n",
    "    return copy.deepcopy(test_preds)\n",
    "\n",
    "def get_metrics_report(train, test, test_preds, test_labels, \n",
    "                       train_path, test_path, label_field,\n",
    "                       dataset_type, expr_type, num_samples, run_id, report_destpath=None):\n",
    "    metrics_report = classification_report(_TEST_LABELS, test_preds, output_dict=True, digits=4)\n",
    "    run_report = { \n",
    "        \"dataset\" : dataset_type, \n",
    "        \"expr\" : expr_type,\n",
    "        \"num_samples\" : num_samples, \n",
    "        \"run_id\" : run_id\n",
    "    }\n",
    "    metrics_report.update(run_report)\n",
    "\n",
    "    report_str = \"### RNN | Dataset: {0}, Expr: {1}, N{2}, R{3}\\n\".format(\n",
    "        dataset_type, expr_type, num_samples, run_id)\n",
    "    report_str += \"```\\n\"\n",
    "    report_str += \"Train: {0}\\nTest: {1}\\n-------------------------------\\n\".format(\n",
    "        train_path, test_path)\n",
    "    \n",
    "    report_str += \"AUC: {0:.4f}\\n\".format(roc_auc_score(test_labels, test_preds, multi_class='ovr'))\n",
    "    report_str += \"Accuracy:\\t{0:.4f}\\t\\tBalanced Acc: {1:.4f}\\n\".format(\n",
    "        accuracy_score(test_preds, test_labels), balanced_accuracy_score(test_preds, test_labels))\n",
    "    report_str += \"Kappa:\\t{0}\\n\".format(cohen_kappa_score(test_preds, test_labels))\n",
    "    report_str += classification_report(test_labels, test_preds, digits=4)\n",
    "    report_str += \"\\n\"\n",
    "    report_str += \"Train distribution:\\t{0}\\nTest distribution:\\t{1}\\n\".format(\n",
    "        getattr(train, label_field).value_counts().to_dict(),\n",
    "        getattr(test, label_field).value_counts().to_dict())\n",
    "    labels = [0, 1] if dataset_type != DATASET_TYPE_AMAZON else [0, 1,2 , 3, 4]\n",
    "    cm = multilabel_confusion_matrix(test_labels, test_preds, labels=labels)\n",
    "    report_str += \"Confusion matrix:\\n\\t{0}\".format(\n",
    "        \"\\n\\t\".join([\"Label {0}: TP {1}, FP {2}, TN {3}, FN {4}\".format(\n",
    "            labels[i], cm[i][1][1], cm[i][0][1], cm[i][0][0], cm[i][1][0]) for i in range(len(cm))]))\n",
    "    report_str += \"\\n```\\n\\n\"\n",
    "    \n",
    "    if report_destpath is not None:\n",
    "        file_obj = open(report_destpath, 'a')\n",
    "        file_obj.write(report_str)\n",
    "        file_obj.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db45d482-e5e0-4409-99fb-bd4595ae6c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_path(expr_type, num_samples, run_id, dataset_filename, file_ext):\n",
    "    return \"expr_data/{0}/{1}/r{2}/expr_{0}_n{1}_r{2}_{3}.{4}\".format(\n",
    "        expr_type, num_samples, run_id, dataset_filename, file_ext)\n",
    "\n",
    "# RNN Processing\n",
    "def proc_rnn_x(df):\n",
    "    return list(getattr(df, TEXT_FIELD).apply(lambda s : s.split())) \n",
    "\n",
    "def load_glove(train):\n",
    "    x_train = proc_rnn_x(train)\n",
    "    #full_train_vocab = cs224u_utils.get_vocab(x_train)\n",
    "    train_vocab = cs224u_utils.get_vocab(x_train, mincount=2)\n",
    "    glove_embedding, glove_vocab = cs224u_utils.create_pretrained_embedding(GLOVE_LOOKUP, train_vocab)\n",
    "    return glove_embedding, glove_vocab\n",
    "\n",
    "\"\"\"\n",
    "# SST-2\n",
    "DATASET_FILENAME = \"sst2_train\"\n",
    "DATASET_TYPE = \"sst2\"\n",
    "FILE_EXT = \"tsv\"\n",
    "LABEL_FIELD = \"label\"\n",
    "TEXT_FIELD = \"sentence\"\n",
    "TEST_PATH = \"expr_data/devtest/{0}_test.{1}\".format(DATASET_TYPE, FILE_EXT)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# IMDB\n",
    "DATASET_FILENAME = \"imdb\"\n",
    "DATASET_TYPE = \"imdb\"\n",
    "FILE_EXT = \"csv\"\n",
    "LABEL_FIELD = \"sentiment\"\n",
    "TEXT_FIELD = \"review\"\n",
    "TEST_PATH = \"expr_data/devtest/{0}_test.{1}\".format(DATASET_FILENAME, FILE_EXT)\n",
    "\"\"\"\n",
    "\n",
    "#\"\"\"\n",
    "# Amazon Reviews\n",
    "DATASET_FILENAME = \"amazon_reviews_digital_music\"\n",
    "DATASET_TYPE = \"amazon\"\n",
    "FILE_EXT = \"json\"\n",
    "LABEL_FIELD = \"overall\"\n",
    "TEXT_FIELD = \"reviewText\"\n",
    "OUTPUT_RESLTS_PATH = \"results/amazon_results.json\"\n",
    "TEST_PATH = \"expr_data/devtest/{0}_test.{1}\".format(DATASET_FILENAME, FILE_EXT)\n",
    "#\"\"\"\n",
    "\n",
    "\n",
    "TEST = dataset_io.to_df(TEST_PATH)\n",
    "_TEST_LABELS = normalize_labels(TEST, LABEL_FIELD)\n",
    "NUM_CLASSES = len(set(getattr(TEST, LABEL_FIELD)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd59cc5-c929-4fc4-8936-cb742be16e1c",
   "metadata": {},
   "source": [
    "### Experiment Changes Start Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40f50e4c-9f07-4c5a-a6f4-60a9f5926dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics report will be written to results/amazon_10_rnn_report.md\n"
     ]
    }
   ],
   "source": [
    "# NUM_SAMPLES can be 10 or 50.\n",
    "NUM_SAMPLES = 10\n",
    "\n",
    "RESULTS_DIR = \"results\"\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    os.mkdir(RESULTS_DIR)\n",
    "\n",
    "REPORT_RESULTS_PATH = \"{0}/{1}_{2}_rnn_report.md\".format(RESULTS_DIR, DATASET_TYPE, NUM_SAMPLES)\n",
    "\n",
    "if not os.path.exists(REPORT_RESULTS_PATH):\n",
    "    open(REPORT_RESULTS_PATH, 'a').close()\n",
    "    print(\"Metrics report will be written to {0}\".format(REPORT_RESULTS_PATH))\n",
    "else:\n",
    "    print(\"Metrics report will be appended to {0}\".format(REPORT_RESULTS_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "201dc8c8-4743-4d05-92b7-b4b8b6253f56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Running amazon experiment on orig, N10, run# 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'cs224u_utils' has no attribute 'progress_bar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a452b10d2748>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         print(\"\\n\\nRunning {0} experiment on {1}, N{2}, run# {3}\".format(\n\u001b[1;32m     11\u001b[0m             DATASET_TYPE, expr_type, NUM_SAMPLES, run_id))\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEXT_FIELD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLABEL_FIELD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_TEST_LABELS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATASET_TYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         get_metrics_report(train, TEST, test_preds, _TEST_LABELS, \n\u001b[1;32m     14\u001b[0m                           \u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEST_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLABEL_FIELD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-470aa5930109>\u001b[0m in \u001b[0;36mrun_and_eval\u001b[0;34m(train, test, text_field, label_field, test_labels, num_classes, dataset_type)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Evaluate on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     rnn_experiment = sst.experiment(\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0msimple_leaves_phi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scu/xcs224u/project/prose_kaleidoscopes/sst.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(train_dataframes, phi, train_func, assess_dataframes, train_size, score_func, vectorize, verbose, random_state, text_field, label_field)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;31m# Train:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m# Predictions if we have labels:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-813787131d38>\u001b[0m in \u001b[0;36mfit_rnn_classifier\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         eta=0.005)\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mbasemod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbasemod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scu/xcs224u/project/prose_kaleidoscopes/torch_model_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    403\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m             cs224u_utils.progress_bar(\n\u001b[0m\u001b[1;32m    406\u001b[0m                 \"Finished epoch {} of {}; error is {}\".format(\n\u001b[1;32m    407\u001b[0m                     iteration, self.max_iter, epoch_error),\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cs224u_utils' has no attribute 'progress_bar'"
     ]
    }
   ],
   "source": [
    "RUN_IDS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "EXPR_TYPES =  [\"orig\", \"para\", \"para_tc\", \"para_editdist\", \"para_tc_editdist\", \"tc\", \"tc_editdist\", \"eda\", \"bt\"]\n",
    "\n",
    "for expr_type in EXPR_TYPES:\n",
    "    for run_id in RUN_IDS:\n",
    "        train_path = get_train_path(expr_type, NUM_SAMPLES, run_id, DATASET_FILENAME, FILE_EXT)\n",
    "        train = dataset_io.to_df(train_path)\n",
    "        GLOVE_EMBEDDING, GLOVE_VOCAB = load_glove(train)\n",
    "        train_labels = normalize_labels(train, LABEL_FIELD)\n",
    "        print(\"\\n\\nRunning {0} experiment on {1}, N{2}, run# {3}\".format(\n",
    "            DATASET_TYPE, expr_type, NUM_SAMPLES, run_id))\n",
    "        test_preds = run_and_eval(train, TEST, TEXT_FIELD, LABEL_FIELD, _TEST_LABELS, NUM_CLASSES, DATASET_TYPE)\n",
    "        get_metrics_report(train, TEST, test_preds, _TEST_LABELS, \n",
    "                          train_path, TEST_PATH, LABEL_FIELD, \n",
    "                          DATASET_TYPE, expr_type, NUM_SAMPLES, run_id, REPORT_RESULTS_PATH)\n",
    "\n",
    "print(\"Finished. Please see the experiment metrics report at {0}\".format(REPORT_RESULTS_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda03d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
