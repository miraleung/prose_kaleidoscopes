{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9a40b8e-531e-47d9-875f-61739fa11c51",
   "metadata": {},
   "source": [
    "# Prose Kaleidoscopes - BERT Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe904620-a9d7-4428-9689-b54022acb946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import copy\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# If Jupyter complains that torch is an unknown module, try:\n",
    "# conda install pytorch torchvision -c pytorch\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "from transformers import BertModel\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Own files\n",
    "import dataset_io\n",
    "\n",
    "# These params should never change\n",
    "NUM_EPOCHS = 4 \n",
    "SEED = 42\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 4e-5\n",
    "NUM_WARMUP_STEPS = 100\n",
    "MAX_SEQ_LEN = 512\n",
    "\n",
    "# Constants\n",
    "DATASET_TYPE_IMDB = \"imdb\"\n",
    "DATASET_TYPE_AMAZON = \"amazon\"\n",
    "DATASET_TYPE_SST2 = \"sst2\"\n",
    "\n",
    "# Experiment parameters.\n",
    "IS_FINE_TUNING = False\n",
    "FREEZE_BERT = IS_FINE_TUNING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45df3033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer\n",
    "BERT_TOKENIZER = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Set up GPU for training.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cef51b2-9e4c-493f-b54c-1ae6c613fe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_list_labels(lst, dataset_type):\n",
    "    if dataset_type == DATASET_TYPE_IMDB:\n",
    "        # Labels are either \"positive\" or \"negative\"\n",
    "        return [0 if v == \"negative\" else 1 for v in lst]\n",
    "    if dataset_type == DATASET_TYPE_AMAZON:\n",
    "        # Labels are in the range [1, 5]\n",
    "        return [(v - 1) * (1.0 / 5) for v in lst]\n",
    "    # SST-2 are 0 (negative) or 1 (positive).\n",
    "    return lst\n",
    "\n",
    "def normalize_labels(df, label_field):\n",
    "    labels = getattr(df, label_field).to_numpy()\n",
    "    # Fix IMDB labels.\n",
    "    is_imdb = len(set(labels)) == 2 and not str(labels[0]).isdigit() and \"positive\" in labels and \"negative\" in labels\n",
    "    # Fix Amazon labels.\n",
    "    is_amazon = len(set(labels)) == 5 and min(set(labels)) == 1 and max(set(labels)) == 5\n",
    "    if is_imdb:\n",
    "        return pd.get_dummies(labels)[\"positive\"]  # 0 is neg, 1 is pos\n",
    "    if is_amazon:\n",
    "        return labels - 1\n",
    "    return labels\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    max_len = MAX_SEQ_LEN\n",
    "    \n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "              \n",
    "        encoded_sent = BERT_TOKENIZER.encode_plus(\n",
    "            text=sent, \n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=max_len,             # Max length to truncate/pad\n",
    "            truncation=True,\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "\n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "def create_dataloader(inputs, masks, labels, is_val=False):\n",
    "    data = TensorDataset(inputs, masks, labels)\n",
    "    sampler = SequentialSampler(data) if is_val else RandomSampler(data) \n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=BATCH_SIZE)\n",
    "    return data, sampler, dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4d2062-9c65-474b-adfb-5eb720ff50ea",
   "metadata": {},
   "source": [
    "## Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74cbe1a5-4eee-4a53-be2f-60a45fea807a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30 µs, sys: 1 µs, total: 31 µs\n",
      "Wall time: 33.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, num_classes\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        #self.bert = BertForSequenceClassification.from_pretrained(\n",
    "        #    'bert-base-uncased', num_labels=num_classes, ignore_mismatched_sizes=True)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.bert.cuda()\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "        self.sig = torch.sigmoid\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "        logits = self.sig(logits)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04a0c060-655a-44dc-8988-35a17dc27827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and Learning Rate Scheduler\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(num_classes, train_dataloader, epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(num_classes, freeze_bert=FREEZE_BERT)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      #lr=5e-5,    # Default learning rate\n",
    "                      lr=LEARNING_RATE,\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    print(\"len(train): {0}, epochs: {1}, total_steps: {2}\".format(len(train_dataloader), epochs, total_steps))\n",
    "    #total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                #num_warmup_steps=0, # Default value\n",
    "                                                num_warmup_steps=NUM_WARMUP_STEPS, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ba059f1-5ffe-4081-ace1-b5c20afa030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Evaluate\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train_bert(model, train_dataloader, optimizer, scheduler, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5561ef09-af6a-45b8-b464-4c2337e7cf4e",
   "metadata": {},
   "source": [
    "## Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "473910e3-343c-43af-bd22-3bc2101ac33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    batch_i = 0\n",
    "    dl_len = len(test_dataloader)\n",
    "    num_eq = 70\n",
    "    for batch in test_dataloader:\n",
    "        if batch_i % 10 == 0:\n",
    "            num_prog = int(num_eq * batch_i / dl_len)\n",
    "            sys.stdout.write(\"Predicting batch {0} / {1} [{2}{3}]\\r\".format(\n",
    "                batch_i, dl_len, \"=\" * num_prog, \".\" * (num_eq - num_prog)))\n",
    "            sys.stdout.flush()\n",
    "        batch_i += 1\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18dad1d",
   "metadata": {},
   "source": [
    "## Metric Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52eaf77d-e6e3-4689-99cd-f6ee239f3ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "def get_roc_metrics(probs, y_true, num_classes, dataset_type):\n",
    "    preds = np.array(probs)\n",
    "    if dataset_type != DATASET_TYPE_AMAZON:\n",
    "        fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Get accuracy over the test set\n",
    "        y_pred = np.where(preds >= 1.0 / num_classes, 1, 0)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        return { \n",
    "            'fpr' : fpr, \n",
    "            'tpr' : tpr, \n",
    "            'roc_auc' : roc_auc, \n",
    "            'accuracy' : accuracy\n",
    "        }\n",
    "    \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    # Force any empty categories to be present\n",
    "    cat_preds = pd.DataFrame(preds)\n",
    "    cat_preds = cat_preds.astype(pd.CategoricalDtype(categories=list([round(i / num_classes, 1) for i in range(num_classes)])))\n",
    "    y_true = pd.get_dummies(y_true).to_numpy()\n",
    "    #preds = pd.get_dummies(cat_preds).to_numpy()\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], preds[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), preds.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(num_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= num_classes\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "    if dataset_type == DATASET_TYPE_AMAZON:\n",
    "        y_pred = preds\n",
    "        row_maxes = preds.max(axis=1).reshape(-1, 1)\n",
    "        y_pred[:] = np.where(y_pred == row_maxes, 1, 0)\n",
    "    else:\n",
    "        y_pred = np.where(preds >= 1.0 / num_classes, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    metrics = {'accuracy' : accuracy}\n",
    "    for k in fpr.keys():\n",
    "        metrics[\"fpr_{0}\".format(k)] = fpr[k]\n",
    "        metrics[\"tpr_{0}\".format(k)] = tpr[k]\n",
    "        metrics[\"roc_auc_{0}\".format(k)] = roc_auc[k]\n",
    "    return metrics\n",
    "    \n",
    "def evaluate_roc_twoclass(probs, y_true, num_classes, dataset_type):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    preds = [np.argmax(p) for p in probs]   \n",
    "    fpr, tpr, roc_auc, accuracy = list(get_roc_metrics(preds, y_true, num_classes, dataset_type).values())\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "    print(f'Accuracy: {accuracy*100:.4f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    \n",
    "def evaluate_roc_multiclass(probs, y_true, num_classes, dataset_type):\n",
    "    metrics = get_roc_metrics(probs, y_true, num_classes, dataset_type)\n",
    "    ks = [\"macro\", \"micro\"] + [i for i in range(num_classes)]\n",
    "    fpr = { k : metrics[\"fpr_{0}\".format(k)] for k in ks }\n",
    "    tpr = { k : metrics[\"tpr_{0}\".format(k)] for k in ks }\n",
    "    roc_auc = { k : metrics[\"roc_auc_{0}\".format(k)] for k in ks }\n",
    "    accuracy = metrics['accuracy']\n",
    "    \n",
    "    print(f'AUC: {0}', roc_auc)\n",
    "    print(f'Accuracy: {accuracy*100:.4f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"], 'b', label = 'Micro-avg AUC = %0.2f' % roc_auc[\"micro\"], color = 'navy')\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"], 'b', label = 'Macro-avg AUC = %0.2f' % roc_auc[\"macro\"], color = 'darkviolet')\n",
    "    colors = ['orange', 'forestgreen', 'cornflowerblue', 'darkgoldenrod', 'tomato', 'dodgerblue']\n",
    "    lw=2\n",
    "    for i, color in zip(range(num_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw, label='AUC of class {0} = {1:0.2f})'.format(i, roc_auc[i]))\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_roc(probs, y_true, num_classes, dataset_type):\n",
    "    ys = pd.get_dummies(y_true)\n",
    "    if dataset_type == DATASET_TYPE_AMAZON:\n",
    "        evaluate_roc_multiclass(probs, ys, num_classes, dataset_type)\n",
    "    else:\n",
    "        evaluate_roc_twoclass(probs, y_true, num_classes, dataset_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33614f2e-2bdc-4a59-bd00-cc93e933800b",
   "metadata": {},
   "source": [
    "## Prediction and More Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea4c1272-7362-44a6-ad6b-48fb4bb0ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation and Metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, cohen_kappa_score, roc_auc_score\n",
    "\n",
    "\n",
    "def get_pred(value, num_classes):\n",
    "    interval = 1.0 / num_classes\n",
    "    for i in range(0, num_classes):\n",
    "        if value < (i + 1) * interval:\n",
    "            return i\n",
    "    return num_classes - 1\n",
    "\n",
    "def run_and_eval(train_dataloader, dev_dataloader, test_dataloader, test_labels, num_classes, dataset_type):\n",
    "    set_seed(SEED)    # Set seed for reproducibility\n",
    "    bert_classifier, optimizer, scheduler = initialize_model(\n",
    "        num_classes, train_dataloader, epochs=NUM_EPOCHS)\n",
    "    train_bert(bert_classifier, train_dataloader,\n",
    "               optimizer, scheduler, \n",
    "               val_dataloader=dev_dataloader, epochs=NUM_EPOCHS, evaluation=True)\n",
    "    \n",
    "    # Compute predicted probabilities on the test set.\n",
    "    test_probs = bert_predict(bert_classifier, test_dataloader)\n",
    "    evaluate_roc(test_probs, test_labels, num_classes, dataset_type)\n",
    "    \n",
    "    if dataset_type == DATASET_TYPE_AMAZON:\n",
    "        test_preds = copy.deepcopy(test_probs)\n",
    "        row_maxes = test_probs.max(axis=1).reshape(-1, 1)\n",
    "        test_preds[:] = np.where(test_preds == row_maxes, 1, 0)\n",
    "        num_rows = test_preds.shape[0]\n",
    "        test_preds = [np.argmax(test_preds[r]) for r in range(num_rows)]\n",
    "    else:\n",
    "        test_preds = np.array([get_pred(p[1], num_classes) for p in test_probs])\n",
    "    return test_preds\n",
    "    \n",
    "\n",
    "def get_metrics_report(train, test, test_preds, test_labels, \n",
    "                       train_path, test_path, label_field,\n",
    "                       dataset_type, expr_type, num_samples, run_id, report_destpath=None):\n",
    "    metrics_report = classification_report(test_labels, test_preds, output_dict=True, digits=4)\n",
    "    run_report = { \n",
    "        \"dataset\" : dataset_type, \n",
    "        \"expr\" : expr_type,\n",
    "        \"num_samples\" : num_samples, \n",
    "        \"run_id\" : run_id\n",
    "    }\n",
    "    metrics_report.update(run_report)\n",
    "    report_str = \"### BERT | Dataset: {0}, Expr: {1}, N{2}, R{3}\\n\".format(\n",
    "        dataset_type, expr_type, num_samples, run_id)\n",
    "    report_str += \"```\\n\"\n",
    "    report_str += \"Train: {0}\\nTest: {1}\\n-------------------------------\\n\".format(\n",
    "        train_path, test_path)\n",
    "    \n",
    "    if dataset_type != DATASET_TYPE_AMAZON:\n",
    "            report_str += \"AUC: {0:.4f}\\n\".format(roc_auc_score(test_labels, test_preds, multi_class='ovr'))\n",
    "    report_str += \"Accuracy:\\t{0:.4f}\\t\\tBalanced Acc: {1:.4f}\\n\".format(\n",
    "        accuracy_score(test_preds, test_labels), balanced_accuracy_score(test_preds, test_labels))\n",
    "    report_str += \"Kappa:\\t{0}\\n\".format(cohen_kappa_score(test_preds, test_labels))\n",
    "    report_str += classification_report(test_labels, test_preds, digits=4)\n",
    "    report_str += \"\\n\"\n",
    "    report_str += \"Train distribution:\\t{0}\\nTest distribution:\\t{1}\\n\".format(\n",
    "        getattr(train, label_field).value_counts().to_dict(),\n",
    "        getattr(test, label_field).value_counts().to_dict())\n",
    "    labels = [0, 1] if dataset_type != DATASET_TYPE_AMAZON else [0, 1,2 , 3, 4]\n",
    "    cm = multilabel_confusion_matrix(test_labels, test_preds, labels=labels)\n",
    "    report_str += \"Confusion matrix:\\n\\t{0}\".format(\n",
    "        \"\\n\\t\".join([\"Label {0}: TP {1}, FP {2}, TN {3}, FN {4}\".format(\n",
    "            labels[i], cm[i][1][1], cm[i][0][1], cm[i][0][0], cm[i][1][0]) for i in range(len(cm))]))\n",
    "    report_str += \"\\n```\\n\\n\"\n",
    "    \n",
    "    if report_destpath is not None:\n",
    "        file_obj = open(report_destpath, 'a')\n",
    "        file_obj.write(report_str)\n",
    "        file_obj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7051547c",
   "metadata": {},
   "source": [
    "## Experiment Parameters\n",
    "\n",
    "Uncomment the appropriate block for the dataset you wish to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcd9705f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/miniconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def get_train_path(expr_type, num_samples, run_id, dataset_filename, file_ext):\n",
    "    return \"expr_data/{0}/{1}/r{2}/expr_{0}_n{1}_r{2}_{3}.{4}\".format(\n",
    "        expr_type, num_samples, run_id, dataset_filename, file_ext)\n",
    "\n",
    "\"\"\"\n",
    "# SST-2\n",
    "DATASET_FILENAME = \"sst2_train\"\n",
    "DATASET_TYPE = \"sst2\"\n",
    "FILE_EXT = \"tsv\"\n",
    "LABEL_FIELD = \"label\"\n",
    "TEXT_FIELD = \"sentence\"\n",
    "DEV_PATH = \"expr_data/devtest/sst2_dev.tsv\"\n",
    "TEST_PATH = \"expr_data/devtest/{0}_test.{1}\".format(DATASET_TYPE, FILE_EXT)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# IMDB\n",
    "DATASET_FILENAME = \"imdb\"\n",
    "DATASET_TYPE = \"imdb\"\n",
    "FILE_EXT = \"csv\"\n",
    "LABEL_FIELD = \"sentiment\"\n",
    "TEXT_FIELD = \"review\"\n",
    "DEV_PATH = \"expr_data/devtest/imdb_dev.csv\"\n",
    "TEST_PATH = \"expr_data/devtest/{0}_test.{1}\".format(DATASET_FILENAME, FILE_EXT)\n",
    "\"\"\"\n",
    "\n",
    "#\"\"\"\n",
    "# Amazon Reviews\n",
    "DATASET_FILENAME = \"amazon_reviews_digital_music\"\n",
    "DATASET_TYPE = \"amazon\"\n",
    "FILE_EXT = \"json\"\n",
    "LABEL_FIELD = \"overall\"\n",
    "TEXT_FIELD = \"reviewText\"\n",
    "DEV_PATH = \"expr_data/devtest/amazon_reviews_digital_music_dev.json\"\n",
    "TEST_PATH = \"expr_data/devtest/{0}_test.{1}\".format(DATASET_FILENAME, FILE_EXT)\n",
    "#\"\"\"\n",
    "\n",
    "\n",
    "DEV = dataset_io.to_df(DEV_PATH)\n",
    "_DEV_LABELS = normalize_labels(DEV, LABEL_FIELD)\n",
    "TEST = dataset_io.to_df(TEST_PATH)\n",
    "_TEST_LABELS = normalize_labels(TEST, LABEL_FIELD)\n",
    "NUM_CLASSES = len(set(getattr(DEV, LABEL_FIELD)))\n",
    "\n",
    "DEV_INPUTS, DEV_MASKS = preprocessing_for_bert(getattr(DEV, TEXT_FIELD).to_numpy())\n",
    "TEST_INPUTS, TEST_MASKS = preprocessing_for_bert(getattr(TEST, TEXT_FIELD).to_numpy())\n",
    "\n",
    "DEV_LABELS = torch.tensor(_DEV_LABELS, dtype=torch.long)\n",
    "TEST_LABELS = torch.tensor(_TEST_LABELS, dtype=torch.long)\n",
    "\n",
    "DEV_DATA, DEV_SAMPLER, DEV_DATALOADER = create_dataloader(DEV_INPUTS, DEV_MASKS, DEV_LABELS, is_val=True)\n",
    "TEST_DATA, TEST_SAMPLER, TEST_DATALOADER = create_dataloader(TEST_INPUTS, TEST_MASKS, TEST_LABELS, is_val=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158033bb-e2cb-4c6c-a5ed-d5559e5da77e",
   "metadata": {},
   "source": [
    "### Change the number of samples here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86f87a07-2532-4611-ac06-7917694d09cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics report will be appended to results/amazon_10_bert_report.md\n"
     ]
    }
   ],
   "source": [
    "# NUM_SAMPLES can be 10 or 50.\n",
    "NUM_SAMPLES = 10\n",
    "\n",
    "RESULTS_DIR = \"results\"\n",
    "REPORT_RESULTS_PATH = \"{0}/{1}_{2}_bert_report.md\".format(RESULTS_DIR, DATASET_TYPE, NUM_SAMPLES)\n",
    "\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    os.mkdir(RESULTS_DIR)\n",
    "\n",
    "if not os.path.exists(REPORT_RESULTS_PATH):\n",
    "    open(REPORT_RESULTS_PATH, 'a').close()\n",
    "    print(\"Metrics report will be written to {0}\".format(REPORT_RESULTS_PATH))\n",
    "else:\n",
    "    print(\"Metrics report will be appended to {0}\".format(REPORT_RESULTS_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "095627a2-7382-47c2-92fb-6d544b556971",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/miniconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Running amazon experiment on orig, N10, run# 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train): 7, epochs: 4, total_steps: 28\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-89eecd7b2c62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         print(\"\\n\\nRunning {0} experiment on {1}, N{2}, run# {3}\".format(\n\u001b[1;32m     16\u001b[0m             DATASET_TYPE, expr_type, NUM_SAMPLES, run_id))\n\u001b[0;32m---> 17\u001b[0;31m         test_preds = run_and_eval(train_dataloader, DEV_DATALOADER, TEST_DATALOADER, \n\u001b[0m\u001b[1;32m     18\u001b[0m                                   _TEST_LABELS, NUM_CLASSES, DATASET_TYPE)\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-e449650e2e79>\u001b[0m in \u001b[0;36mrun_and_eval\u001b[0;34m(train_dataloader, dev_dataloader, test_dataloader, test_labels, num_classes, dataset_type)\u001b[0m\n\u001b[1;32m     15\u001b[0m     bert_classifier, optimizer, scheduler = initialize_model(\n\u001b[1;32m     16\u001b[0m         num_classes, train_dataloader, epochs=NUM_EPOCHS)\n\u001b[0;32m---> 17\u001b[0;31m     train_bert(bert_classifier, train_dataloader,\n\u001b[0m\u001b[1;32m     18\u001b[0m                \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                val_dataloader=dev_dataloader, epochs=NUM_EPOCHS, evaluation=True)\n",
      "\u001b[0;32m<ipython-input-7-c73d5995671a>\u001b[0m in \u001b[0;36mtrain_bert\u001b[0;34m(model, train_dataloader, optimizer, scheduler, val_dataloader, epochs, evaluation)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m# Perform a forward pass. This will return logits.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_attn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# Compute loss and accumulate the loss values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n",
      "\u001b[0;32m/Applications/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/miniconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m         )\n\u001b[0;32m--> 995\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    996\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/miniconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    580\u001b[0m                 )\n\u001b[1;32m    581\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    583\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/miniconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/miniconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     ):\n\u001b[0;32m--> 401\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/miniconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "RUN_IDS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "EXPR_TYPES = [\"orig\", \"para\", \"para_tc\", \"para_editdist\", \"para_tc_editdist\", \"tc\", \"tc_editdist\", \"eda\", \"bt\"]\n",
    "\n",
    "for expr_type in EXPR_TYPES:\n",
    "    for run_id in RUN_IDS:\n",
    "        train_path = get_train_path(expr_type, NUM_SAMPLES, run_id, DATASET_FILENAME, FILE_EXT)\n",
    "        train = dataset_io.to_df(train_path)\n",
    "        train_inputs, train_masks = preprocessing_for_bert(getattr(train, TEXT_FIELD).to_numpy())\n",
    "        \n",
    "        _train_labels = normalize_labels(train, LABEL_FIELD)\n",
    "        train_labels = torch.tensor(_train_labels, dtype=torch.long)\n",
    "        train_data, train_sampler, train_dataloader = create_dataloader(train_inputs, train_masks, train_labels)\n",
    "        \n",
    "        # Train and get metrics.\n",
    "        print(\"\\n\\nRunning {0} experiment on {1}, N{2}, run #{3}\".format(\n",
    "            DATASET_TYPE, expr_type, NUM_SAMPLES, run_id))\n",
    "        test_preds = run_and_eval(train_dataloader, DEV_DATALOADER, TEST_DATALOADER, \n",
    "                                  _TEST_LABELS, NUM_CLASSES, DATASET_TYPE)\n",
    "\n",
    "        get_metrics_report(train, TEST, test_preds, _TEST_LABELS, \n",
    "                          train_path, TEST_PATH, LABEL_FIELD, \n",
    "                          DATASET_TYPE, expr_type, NUM_SAMPLES, run_id, REPORT_RESULTS_PATH)\n",
    "\n",
    "print(\"Finished. Please see the experiment metrics report at {0}\".format(REPORT_RESULTS_PATH))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
