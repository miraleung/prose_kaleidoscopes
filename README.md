# Prose Kaleidoscopes

Maximally-diverse data augmentation techniques for NLP. These are analogous to looking at a piece of text from different perspectives, akin to a *prose kaleidoscope*.

This codebase consists of two main parts - the experiment source code and processed data, and scripts for you to augment your own datasets.

## Experiment Setup
1. Install dependencies.
    ```
    pip install -r requirements.txt
    ```
2. Open notebooks and run the experiments. All the low-resource data has been pre-processed and is already included in `expr_data/`.


## Data Augmentation
The following steps will normalize the data, extract a low-resource sample, and
augment it with our maximally-diverse methods and the baseline approaches (backtranslation and EDA).
Many of these steps are orthogonal and do not necessarily need to be done in order.

The processed data in `expr_data` was generated by applying the following
steps many times to the [SST-2](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip),
[Amazon digital music reviews](https://jmcauley.ucsd.edu/data/amazon/), and
[IMDB movie review](https://ai.stanford.edu/~amaas/data/sentiment/) datasets.

1. Normalize the dataset. This will also tag each data point with its original
   position in the full dataset. Example:
   ```
   python normalizer_main.py \
       --in data/sst2_train.tsv \
       --out /tmp/normalized_sst2_train.tsv \
       --field sentence
   ```

2. Extract a small subsample to simulate a low-resource environment. Here, we will
   randomly sample 20 data points from our normalized dataset. There are two classes
   in this dataset, so passing in `--num_samples=10` will give us 20 samples in total.

   This step is optional if you would like to augment the entire dataset.
   ```
   python sample_extractor_main.py \
       --in /tmp/normalized_sst2_train.tsv \
       --out /tmp/sampled_orig_sst2_train.tsv \
       --label_field label \
       --num_samples 10
   ```

   In the rest of the transformations below, the first line will always be the
   original, unaugmented text.

3. Generate a version of our low-resource dataset that is augmented by way of paraphrasing the datapoints.
   ```
   python paraphraser_main.py \
       --in /tmp/sampled_orig_sst2_train.tsv \
       --out /tmp/sampled_para_sst2_train.tsv \
       --field sentence
   ```

4. Perform edit-distance filtering on our paraphrase-augmented dataset.
   Here, we set the minimum edit distance threshold to 35% of a data point's
   character length.
   ```
   python farthest_main.py \
       --in /tmp/sampled_para_sst2_train.tsv \
       --out /tmp/sampled_para_editdist_sst2_train.tsv \
       --field sentence \
       --thres_p 0.35
   ```

5. Apply translation chain augmentation to the paraphrase-augmented dataset, and to the original dataset.
   To try this without an API key, simply do not pass the `--use_actual` flag to the command, although
   there is no guarantee this will work (more details on the underlying dependency [here](https://pypi.org/project/googletrans/)).
   An API key is the best way to ensure that translations will be successful. Please follow
   [these steps](https://cloud.google.com/translate/docs/setup) to set up your Google API
   key and credentials.

   If you'd like to avoid translating some data points twice, simply translate the
   paraphrase-augmented file, then reconstruct the translation of the original, unaugmented dataset
   by extracting the first four lines of each data point. Since the first line in any `orig_index`
   group is always the original data point, the subsequent three lines will be its translations.
   The paraphrase-augmented data and its translations will follow this first group.
   ```
   export GOOGLE_APPLICATION_CREDENTIALS="/home/user/Downloads/service-account-file.json"
   python translator_main.py \
       --in /tmp/sampled_orig_sst2_train.tsv \
       --out /tmp/sampled_tc_sst2_train.tsv \
       --field sentence \
       --use_actual  # Pass this in only if you have set up your API key.

   # This will use a capped, free version of Google Translate.
   python translator_main.py \
       --in /tmp/sampled_para_sst2_train.tsv \
       --out /tmp/sampled_para_tc_sst2_train.tsv \
       --field sentence
   ```

   If needed, re-normalize the resulting files.

   ```
   python normalizer_main.py \
       --in /tmp/sampled_tc_sst2_train.tsv \
       --out /tmp/sampled_normalized_tc_sst2_train.tsv \
       --field sentence

   python translator_main.py \
       --in /tmp/sampled_para_tc_sst2_train.tsv \
       --out /tmp/sampled_normalized_para_tc_sst2_train.tsv \
       --field sentence
   ```

6. Perform edit-distance filtering on the translated datasets. For simplicity
   in this example, we will filter only the dataset augmented with paraphrasing
   and translation chains. The command for filtering the one augmented only with
   translation chains is very similar.
   ```
   python farthest_main.py \
       --in /tmp/sampled_normalized_para_tc_sst2_train.tsv  \
       --out /tmp/sampled_normalized_para_tc_editdist_sst2_train.tsv \
       --field sentence \
       --thres_p 0.35
   ```

7. As a baseline, let's augment our original sample with [EDA](https://github.com/jasonwei20/eda_nlp).
   ```
   pyton eda_augmenter_main.py \
       --in /tmp/sampled_orig_sst2_train.tsv \
       --out /tmp/sampled_eda_sst2_train.tsv \
       --field sentence
   ```

8. As a second baseline, let's apply backtranslation to our original dataset. Please see
   the translation chain step above for details on setting up the Google Translate API.
   ```
   python backtranslator_main.py \
       --in /tmp/sampled_orig_sst2_train.tsv \
       --out /tmp/sampled_bt_sst2_train.tsv \
       --field sentence
       --use_actual  # Pass this in only if you have set up your API key.
   ```
